{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fake-news-unal/fake-news/blob/main/K_means%2Clogistica%2Cbosque%2Carbol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeHEFt2BPnph"
   },
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# Importar keras\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Para la matrix de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_db = \"../dataset/fake_news.sqlite\"\n",
    "\n",
    "conn = sqlite3.connect(sqlite_db)\n",
    "df = pd.read_sql_query(\"SELECT * FROM NEWS\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['original'] = df_news['title'] + '. ' + df_news['text']\n",
    "df_news['original'].replace(\"..\", \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Total de palabras en el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "unique_words = set()\n",
    "for document in df_news.clean_joined:\n",
    "    for word in gensim.utils.simple_preprocess(document):\n",
    "        list_of_words.append(word)\n",
    "        unique_words.add(word)\n",
    "        \n",
    "total_words = len(list_of_words)  # total words\n",
    "unique_words = len(unique_words)   # total unique words\n",
    "print(\"Total words:\" + str(total_words) + \" unique_words:\" + str(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "janhB5xjRxD6"
   },
   "source": [
    "### Partición del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3fnkfll7b7t"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QeGUuv67b5J"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = total_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uly4JzYL7Q1C",
    "outputId": "3abcc51b-9f10-40cd-c508-e0e1c1decf50"
   },
   "outputs": [],
   "source": [
    "print(\"Texto original (limpio) \\n\",df.clean_joined[0],\"\\n Texto Tokenizado : \",train_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A12wC2DwS0dq"
   },
   "source": [
    "### Word2vec (Vectorización)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XU7HN1P89H2"
   },
   "outputs": [],
   "source": [
    "#Importando librerías\n",
    "import string # python library\n",
    "import re # regex library\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short # Preprocesssing\n",
    "from gensim.models import Word2Vec # Word2vec\n",
    "\n",
    "from sklearn import cluster # Kmeans clustering\n",
    "from sklearn import metrics # Metrics for evaluation\n",
    "from sklearn.decomposition import PCA #PCA\n",
    "from sklearn.manifold import TSNE #TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A65ekfleEcKY"
   },
   "outputs": [],
   "source": [
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short]\n",
    "\n",
    "processed_data = []\n",
    "processed_labels = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    words_broken_up = preprocess_string(row['original'], CUSTOM_FILTERS)\n",
    "    # Esto elimina cualquier campo que pueda quedar en blanco después del preprocesamiento.\n",
    "    if len(words_broken_up) > 0:\n",
    "        processed_data.append(words_broken_up)\n",
    "        processed_labels.append(row['isfake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHKF3-7w7GQc"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(processed_data, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t77Nm3st0qTm",
    "outputId": "c4c14060-2a33-4330-a8b1-29f5c2cc37a2"
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErBa0iicBZ8Q",
    "outputId": "a7e65dc2-3d72-4467-a63d-1e41f5bf6570"
   },
   "outputs": [],
   "source": [
    "# Obtener el vector de una oración basado en el promedio de todos los vectores de palabras en la oración\n",
    "# Obtenemos el promedio ya que esto representa diferentes longitudes de oraciones\n",
    "def ReturnVector(x):\n",
    "    try:\n",
    "        return model[x]\n",
    "    except:\n",
    "        return np.zeros(100)\n",
    "    \n",
    "def Sentence_Vector(sentence):\n",
    "    word_vectors = list(map(lambda x: ReturnVector(x), sentence))\n",
    "    return np.average(word_vectors, axis=0).tolist()\n",
    "\n",
    "X = []\n",
    "for data_x in processed_data:\n",
    "    X.append(Sentence_Vector(data_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctxx4okg9m-s",
    "outputId": "3be2fce9-1e6e-4485-e9a1-5f3f237e901a"
   },
   "outputs": [],
   "source": [
    "X_np = np.array(X)\n",
    "X_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Lz7H82WU3Y0"
   },
   "source": [
    "# ***Arból de decisión***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2pJGCLVJHNz",
    "outputId": "b1af72a4-2806-49f6-a6f5-ac6cb20dc07c"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKX4krCQVFBF"
   },
   "source": [
    "# **Evaluación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V41SdT3qJHKJ"
   },
   "outputs": [],
   "source": [
    "pred_dt = DT.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKEUbqx4JHHT",
    "outputId": "b45105ed-167e-4f86-8978-eb97196397a2"
   },
   "outputs": [],
   "source": [
    "accuracy1 = accuracy_score(list(y_test), pred_dt)\n",
    "\n",
    "print(\"Model Accuracy : \", accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBNupozYJHAt",
    "outputId": "4209bbfb-fad1-486b-e407-dd3a867f175b"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "PuFlGeM3VLwO",
    "outputId": "24554c19-e98d-428d-8dcb-98a98d8cf39e"
   },
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y_test, pred_dt, labels=[0,1])\n",
    "\n",
    "# group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_names = ['Verdaderos Positivos','Falsos Positivo','Falsos Negativos','Verdaderos Negativos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm2.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cm2.flatten()/np.sum(cm2)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "precision = cm2[1,1] / sum(cm2[:,1])\n",
    "recall    = cm2[1,1] / sum(cm2[1,:])\n",
    "f1_score  = 2*precision*recall / (precision + recall)\n",
    "stats_text = \"\\nAccuracy1={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(accuracy1,precision,recall,f1_score)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm2, annot=labels, fmt='', cmap='Blues', ax = ax); #annot=True to annotate cells\n",
    "\n",
    "ax.set_xlabel('Predicted label' + stats_text)\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel(\"Valores predicción \\n\\n Métricas\" + stats_text)\n",
    "ax.set_ylabel('Valores Reales')\n",
    "ax.set_title('Matriz de confusión')\n",
    "ax.xaxis.set_ticklabels(['Reales(0)', 'Falsas(1)'])\n",
    "ax.yaxis.set_ticklabels(['Reales(0)', 'Falsas(1)'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNYG+kUGdJdXEl3FHkBOfiH",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "K-means,logistica,bosque,arbol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
